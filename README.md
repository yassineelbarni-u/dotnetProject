# ğŸš€ SystÃ¨me RAG + LLM avec Cache Redis

## ğŸ“‹ Vue d'ensemble

Projet ASP.NET Core avec systÃ¨me de recommandations intelligent utilisant :
- **LLM** : Ollama (gemma:2b) pour gÃ©nÃ©ration de rÃ©ponses
- **RAG** : Vector Database (Qdrant) pour recherche sÃ©mantique
- **Cache** : Redis pour optimisation des performances

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Pages Razor (Interface)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     OllamaRecommendationService (LLM)           â”‚
â”‚     â€¢ Orchestration du flux RAG                 â”‚
â”‚     â€¢ GÃ©nÃ©ration rÃ©ponse via Ollama             â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                         â”‚
     â”‚ utilise                 â”‚ cache avec
     â”‚                         â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VectorRAGService   â”‚   â”‚  Redis Cache         â”‚
â”‚  â€¢ Recherche        â”‚   â”‚  â€¢ Cache embeddings  â”‚
â”‚    vectorielle      â”‚   â”‚  â€¢ Cache rÃ©ponses    â”‚
â”‚  â€¢ Indexation       â”‚   â”‚  â€¢ TTL: 30min        â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚
     â”‚            â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Qdrant    â”‚ â”‚  SemanticKernel       â”‚
â”‚  Vector DB  â”‚ â”‚  â€¢ Embeddings (384d)  â”‚
â”‚  Port 6333  â”‚ â”‚  â€¢ Ollama: all-minilm â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Services principaux

### **1. OllamaRecommendationService (LLM)**
```csharp
// Orchestration complÃ¨te du processus RAG
public async Task<string> GetRecommendationsAsync(string userQuery)
{
    // 1. RETRIEVAL : RÃ©cupÃ©rer produits pertinents
    var relevantProducts = _ragService.RetrieveRelevantProducts(userQuery, allProducts);
    
    // 2. AUGMENTATION : Enrichir le prompt
    var enrichedPrompt = BuildPrompt(userQuery, relevantProducts);
    
    // 3. GENERATION : Appeler Ollama
    var response = await CallOllamaAsync(enrichedPrompt);
    
    return response;
}
```

### **2. VectorRAGService (RAG)**
```csharp
// Recherche vectorielle avec Qdrant
public List<Produit> RetrieveRelevantProducts(string query, List<Produit> allProducts)
{
    // 1. Convertir la question en vecteur (embedding)
    var queryEmbedding = _embeddingService.GenerateEmbeddingAsync(query);
    
    // 2. Indexer les produits dans Qdrant (si nÃ©cessaire)
    IndexProductsIfNeeded(allProducts);
    
    // 3. Rechercher les produits similaires dans Qdrant
    var similarIds = _qdrantService.SearchAsync(queryEmbedding, topK: 10);
    
    return allProducts.Where(p => similarIds.Contains(p.Id)).ToList();
}
```

### **3. Redis Cache**
```csharp
// Configuration dans Program.cs
builder.Services.AddStackExchangeRedisCache(options =>
{
    options.Configuration = "localhost:6379";
    options.InstanceName = "ProjetTestDotNet_";
});
```

---

## ğŸ“¦ Installation

### **1. Packages NuGet**
```bash
dotnet add package Microsoft.SemanticKernel --version 1.28.0
dotnet add package Microsoft.SemanticKernel.Connectors.Ollama --version 1.29.0-alpha
```

### **2. Lancer Qdrant (Vector Database)**
```bash
docker-compose up -d
```

### **3. Lancer Redis (si pas dÃ©jÃ  actif)**
```bash
docker run -d -p 6379:6379 --name redis redis:alpine
```

### **4. Installer modÃ¨le Ollama**
```bash
ollama pull gemma:2b
ollama pull all-minilm
```

---

## ğŸš€ Lancer l'application

```bash
dotnet build
dotnet run
```

AccÃ©der : `http://localhost:5000`

---

## ğŸ§ª Test

Page recommandations : `/Recommendations`

Exemples de questions :
- "Produits moins de 50â‚¬"
- "Produit pour dÃ©veloppeur dÃ©butant"
- "Formation en Python"

---

## âš¡ Performance avec Redis

| Sans cache | Avec cache Redis |
|-----------|------------------|
| ~3000ms   | ~50ms           |
| âŒ RequÃªte LLM Ã  chaque fois | âœ… RÃ©ponse instantanÃ©e |

---

## ğŸ”‘ Points clÃ©s

### **Flux RAG complet :**
1. **RETRIEVAL** : VectorRAGService rÃ©cupÃ¨re produits pertinents via Qdrant
2. **AUGMENTATION** : Enrichissement du contexte avec mÃ©tadonnÃ©es produits
3. **GENERATION** : Ollama gÃ©nÃ¨re rÃ©ponse personnalisÃ©e en franÃ§ais
4. **CACHE** : Redis stocke rÃ©ponse (TTL: 30min)

### **Vector Database (Qdrant) :**
- Stockage de vecteurs (embeddings 384 dimensions)
- Recherche par similaritÃ© cosinus
- Dashboard : `http://localhost:6333/dashboard`

### **Cache Redis :**
- RÃ©duction temps rÃ©ponse : 3s â†’ 50ms
- TTL configurable (dÃ©faut: 30 minutes)
- Cache invalidation automatique

---

## ğŸ“‚ Structure services

```
Services/
â”œâ”€â”€ IRAGService.cs                    # Interface RAG
â”œâ”€â”€ VectorRAGService.cs               # RAG avec Vector DB
â”œâ”€â”€ IEmbeddingService.cs              # Interface embeddings
â”œâ”€â”€ SemanticKernelEmbeddingService.cs # GÃ©nÃ©ration embeddings
â”œâ”€â”€ IQdrantService.cs                 # Interface Qdrant
â”œâ”€â”€ QdrantService.cs                  # API REST Qdrant
â””â”€â”€ OllamaRecommendationService.cs    # Service LLM principal
```

---

## ğŸ³ Docker

```yaml
# docker-compose.yml
services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
```

---

## ğŸ“Š Configuration

**appsettings.json :**
```json
{
  "ConnectionStrings": {
    "DefaultConnection": "Server=localhost;Database=ProjetTest;..."
  }
}
```

**Program.cs :**
```csharp
// Services RAG + LLM
builder.Services.AddScoped<IEmbeddingService, SemanticKernelEmbeddingService>();
builder.Services.AddScoped<IQdrantService, QdrantService>();
builder.Services.AddScoped<IRAGService, VectorRAGService>();
builder.Services.AddScoped<IRecommendationService, OllamaRecommendationService>();

// Redis Cache
builder.Services.AddStackExchangeRedisCache(options => {
    options.Configuration = "localhost:6379";
});
```

---

**ğŸ“ Projet prÃªt pour dÃ©monstration professionnelle !**
